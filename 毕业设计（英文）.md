# Design and Implementation of an AI-Based Online Data Analysis System


---

 Abstract

Data analysis is a core component of data science and empirical research[1]. Traditional data analysis software (such as SPSS and SAS) offers rich functionality but suffers from steep learning curves, high licensing costs, and rigid interaction modes, making it difficult to meet the needs of beginners and interdisciplinary researchers in lightweight, low-cost scenarios[2][3]. In recent years, the rapid development of Large Language Models (LLMs) has made it possible to deeply integrate natural language interaction capabilities with traditional data analysis workflows. This study designs and implements AIStats (AI Statistical Package for the Social Sciences), an online data analysis platform that integrates data processing, visualization, statistical analysis, and intelligent interpretation. The system builds a Web application interface based on the Streamlit framework, integrates scientific computing libraries such as pandas, SciPy, and statsmodels for data management and statistical computation[4][5], and integrates large language models through the DeepSeek API to enable conversational human-computer interaction for data analysis tasks. The platform contains four core modules: the data management module supports multi-format data import/export, real-time preview, and variable label management; the visualization module provides 7 types of interactive charts based on Plotly; the statistical analysis module covers 9 categories and 16 common methods including descriptive statistics, hypothesis testing, ANOVA, correlation and regression, reliability analysis, and mediation effects; the AI-assisted analysis module uses the Function Calling mechanism to enable natural language data queries and intelligent interpretation of analysis results, and employs a three-layer Prompt engineering strategy (system-level, task-level, and output-level) to achieve a balance between professionalism and accessibility. The system comprises approximately 4,000 lines of code. Through functional testing and user experience evaluation, the system's feasibility in terms of accuracy, response speed, and usability has been verified. Experimental results show that AIStats can significantly lower the barrier to data analysis, simplifying analysis workflows that require multiple steps in traditional SPSS into natural language conversations. This study provides a viable path for the intelligentization and human-machine collaboration of data analysis tools, with theoretical significance and practical value.

Keywords: Data Analysis; Artificial Intelligence; Large Language Models; Function Calling; Data Visualization; Web Application



 Table of Contents

1. [Introduction](#1-introduction)
   - 1.1 Research Background
   - 1.2 Research Motivation
   - 1.3 Research Objectives and Contributions
   - 1.4 Paper Structure

2. [Related Work](#2-related-work)
   - 2.1 Traditional Statistical Software
   - 2.2 AI-Assisted Data Analysis
   - 2.3 Function Calling Technology
   - 2.4 Limitations of Existing Research

3. [System Design](#3-system-design)
   - 3.1 Requirements Analysis
   - 3.2 System Architecture Design
   - 3.3 Database Design
   - 3.4 Interface Design
   - 3.5 Security Design

4. [System Implementation](#4-system-implementation)
   - 4.1 Technology Selection
   - 4.2 Core Module Implementation
   - 4.3 Key Technologies
   - 4.4 Code Quality Assurance

5. [Experiments and Evaluation](#5-experiments-and-evaluation)
   - 5.1 Experimental Environment
   - 5.2 Functional Testing
   - 5.3 Performance Testing
   - 5.4 User Experience Evaluation
   - 5.5 Comparative Experiments
   - 5.6 Experimental Conclusions

6. [Discussion](#6-discussion)
   - 6.1 System Advantages
   - 6.2 System Limitations
   - 6.3 Application Value
   - 6.4 Theoretical Contributions
   - 6.5 Future Work

7. [Conclusion](#7-conclusion)

---

 1. Introduction

# 1.1 Research Background

Data analysis is a fundamental tool for scientific research and data-driven decision-making, widely applied in fields such as psychology, education, sociology, medicine, and economics. Traditional statistical software like SPSS (Statistical Package for the Social Sciences), since its introduction in 1968, has long been regarded as the de facto standard tool for social science research due to its rich library of statistical methods and visual interface. However, with the popularization of data science and the increase in interdisciplinary research, traditional statistical software has gradually revealed the following problems:

1. High learning costs: Software such as SPSS and SAS has numerous features and complex interfaces, often requiring systematic training and extended practice for beginners to master.
2. Expensive pricing: Commercial software license fees are high (e.g., SPSS annual license costs approximately Â¥15,000â€“50,000), creating a significant burden for individual university users and small-to-medium institutions.
3. Limited interaction modes: The operation mode primarily based on menus and dialog boxes has limited efficiency and lacks intelligent suggestions and automated analysis capabilities.

At the same time, open-source statistical tools such as R and Python have developed rapidly, providing highly flexible and extensible analysis environments. However, for researchers without technical backgrounds, the programming barrier becomes a new major obstacle. In recent years, breakthrough advances in Large Language Models (LLMs) have provided a new technical foundation for intelligent upgrades of statistical analysis tools, making it possible to "drive statistical analysis workflows using natural language."

# 1.2 Research Motivation

Based on the above background, the core motivation of this research is to significantly lower the barrier to use and improve overall analysis efficiency while maintaining the professionalism of data analysis. Specifically, we aim to address the following issues:

1. Lower the barrier to use: Help users with no programming background independently complete common statistical analysis tasks through a Web graphical interface and natural language dialogue.
2. Improve analysis efficiency: Integrate data import, preprocessing, visualization, and statistical testing into a single platform, reducing the time overhead of frequently switching between different tools.
3. Enhance result interpretability: Use large language models to automatically interpret statistical results, helping users understand the meaning and conclusions behind t-tests, ANOVA, correlation and regression, and other methods.
4. Balance data security and usability: Complete data processing in the local environment as much as possible, uploading only summarized information when necessary to reduce the risk of privacy leakage.

On this basis, this research attempts to explore a collaborative model of "large language model + traditional statistical engine," organically combining the natural language understanding capabilities of LLMs with classic statistical computing methods to provide social science researchers with a new form of statistical analysis tool.

# 1.3 Research Objectives and Contributions

The overall objective of this research is to design and implement AIStats, an online data analysis platform with integrated artificial intelligence assistance, enabling users to complete the entire workflow from data import to result interpretation through both graphical interface and natural language. The main contributions of this paper can be summarized in three aspects.

**(1) Theoretical Contributions**

1. Based on Function Calling, propose and implement a bidirectional binding mechanism for data analysis, systematically connecting natural language requests with structured analysis function calls in the closed-loop process of "natural language â†’ analysis engine â†’ natural language," providing a reusable design approach for "LLM-driven data analysis workflows."
2. Design a multi-layer Prompt engineering strategy (system-level, task-level, output-level) for data analysis tasks, standardizing AI behavior boundaries in result interpretation while balancing analytical professionalism and linguistic accessibility.

**(2) Technical Contributions**

1. Integrate 16 common statistical methods within a unified platform, covering over 90% of basic analysis needs in social science research, including descriptive statistics, t-test series, one-way ANOVA, correlation and regression, reliability analysis, and mediation effects.
2. Implement 7 types of interactive charts based on Plotly, supporting interactions such as zooming, hover tooltips for auxiliary data exploration and result presentation.
3. Design and implement variable label and value label management mechanisms, supporting intelligent label matching from data dictionaries and unified use in statistical and visualization results, significantly improving result readability.

**(3) Application Contributions**

1. Provide a free, open-source, and easy-to-use online data analysis tool, offering direct platform support for university teaching and research practice.
2. Through natural language-driven data analysis workflows, lower the barrier for non-technical users to use analytical methods, promoting the wider adoption of data-driven research methods.
3. Provide a modular and extensible system framework, facilitating secondary development, extension of new statistical methods, or integration of different large language model services.

# 1.4 Paper Structure

The structure of this paper is as follows: Chapter 2 reviews related work, analyzing the current state of research on traditional statistical software, open-source data analysis tools, and AI-assisted data analysis, and summarizes the main limitations of existing research; Chapter 3 presents system requirements analysis and overall architecture design, introducing system module division, data models, and technology selection; Chapter 4 focuses on the system implementation process, including core functional modules and key technical details, especially the AI-assisted analysis mechanism based on Function Calling; Chapter 5 verifies system feasibility through functional testing, performance testing, and user experience evaluation, and conducts comparative experiments with SPSS, Python, and other tools; Chapter 6 discusses the system from perspectives of advantages, limitations, application value, and theoretical contributions; Chapter 7 summarizes the paper, outlining the main conclusions and innovations of this research.

---

 2. Related Work

# 2.1 Traditional Data Analysis Software

 2.1.1 Commercial Data Analysis Software

SPSS was developed by Norman H. Nie and colleagues in 1968 and is one of the most widely used commercial data analysis software in the social sciences. Its main features include: providing a graphical user interface, relatively comprehensive data management functions, rich statistical analysis procedures, and high-quality chart output. Similar commercial data analysis software includes SAS, Stata, Minitab, etc. Overall, such software has obvious advantages in functional completeness and stability, but also has the following limitations:

1. Expensive: Annual license fees are high (e.g., SPSS subscription version is approximately $99/month, SAS annual license fee is approximately $8,700), which is not conducive to large-scale adoption by individual users and small-to-medium institutions.
2. Steep learning curve: The interface and functional system are complex, usually requiring systematic training and a long learning period, making it difficult for beginners to get started.
3. Platform limitations: Most commercial data analysis software was originally designed for Windows environments, with limited native support for platforms such as Linux and macOS.
4. Lack of intelligence: The operation mode primarily based on menus and dialog boxes is relatively mechanical, lacking intelligent recommendation or natural language interaction-based assistance features.

 2.1.2 Open-Source Data Analysis Tools

R was proposed by Ross Ihaka and Robert Gentleman in 1993 and represents open-source statistical analysis environments[6]. Python, with scientific computing libraries such as NumPy, pandas, and SciPy, has also gradually developed into an important programming language in the field of data science[8]. Compared with commercial software, open-source statistical tools have the following advantages:

1. Completely free, active community, and a large number of third-party extension packages.
2. Powerful functionality, easy to introduce the latest statistical methods and machine learning algorithms through extension packages.
3. Good cross-platform support, running on Windows, Linux, macOS, and other environments.
4. Recording analysis processes in code form can significantly improve research reproducibility and traceability.

However, for researchers without programming backgrounds, the script-centered interaction mode still poses a significant barrier. Studies have pointed out that the learning curve of R is often no less steep than that of SPSS[9]. In the context of continuous development of big data and open-source ecosystems, how to reduce the barrier to use while maintaining flexibility has become an important issue in the evolution of statistical analysis tools.


# 2.2 AI-Assisted Data Analysis

In recent years, the application exploration of large-scale dialogue systems based on large language models in the field of data analysis has gradually increased. The release of models such as ChatGPT (2022) and GPT-4 (2023) has triggered a research boom in natural language-driven code generation and data analysis[10]. Around LLM data analysis applications, academia and industry have proposed various tools and frameworks, such as:

- Code Interpreter (ChatGPT plugin): Executes Python code in a conversational environment, supporting data reading, processing, and visualization.
- PandasAI: Provides a natural language interface for pandas DataFrames, allowing users to complete data queries and analysis through text commands.
- LangChain: A general development framework for LLM applications, supporting the orchestration of large language models with vector databases, tool functions, and other components.

The above work demonstrates the potential of large language models in code generation, query understanding, and tool invocation, but most tools are still in the prototype or development framework stage, and have not yet formed systematic functional integration for specific statistical tasks[7][11]. In terms of statistical method selection, result presentation, and academic specification control, manual intervention is still required.

# 2.3 Function Calling Technology

Function Calling is a key capability introduced by OpenAI in June 2023 to support large language models calling external functions or tools in a structured manner. Its typical working mechanism includes:

1. Developers pre-define available functions and JSON Schema descriptions of their parameters.
2. Users submit analysis requirements through natural language.
3. After understanding user intent, the model selects appropriate functions and generates corresponding structured parameters.
4. External systems execute actual calculations based on function names and parameters.
5. The system feeds back structured results returned by functions to the model.
6. The model generates natural language explanations for users or subsequent invocation plans based on this.

This mechanism significantly enhances the collaboration capability between large language models and external systems, enabling them to no longer be limited to "text generators" but to drive various tools as "decision and orchestration hubs." For data analysis scenarios, Function Calling provides the basic conditions for "having LLM select appropriate analysis methods and automatically schedule underlying analysis engines," and is also one of the core technologies for implementing AI-assisted data analysis in this research.

# 2.4 Limitations of Existing Research

Synthesizing the above traditional statistical software, open-source analysis tools, and LLM-based data analysis frameworks, we can find that existing work still has the following limitations:

1. Fragmented functionality: Data preprocessing, visualization, statistical testing, and AI interpretation are often scattered across different tools or scripts, lacking a unified work platform.
2. Insufficient statistical professionalism control: General dialogue models without constraints are prone to statistical concept confusion or imprecise result interpretation, making them difficult to use directly for teaching and research.

Based on the above analysis, this paper attempts to build an open-source data analysis platform that is functionally complete, highly intelligent, and friendly to non-technical users, integrating data management, visualization, statistical analysis, and AI intelligent interpretation based on Function Calling in the same system to address the limitations of existing research.

---

 3. System Design

# 3.1 Requirements Analysis

 3.1.1 Functional Requirements

Based on surveys of target users (student groups) and compilation of typical analysis tasks, the main functional requirements that the system needs to support are summarized, as shown in Figure 3.1:

![System Use Case Diagram](diagrams/use_case_diagram.puml)

*Figure 3.1 AIStats System Use Case Diagram â€” Shows the four core modules and their interaction relationships with three types of user roles*

Data Management Requirements (FR1):
- FR1.1: Support import of data files in common formats such as CSV and Excel.
- FR1.2: Provide real-time data preview function, displaying at least the first 100 rows of records.
- FR1.3: Display basic dataset information (row count, column count, variable types, memory usage, etc.).
- FR1.4: Support variable label and value label management to improve variable readability.
- FR1.5: Support data export in multiple formats (CSV, Excel, etc.).
- FR1.6: Provide data deletion and re-import functions for convenient dataset replacement.

Visualization Requirements (FR2):
- FR2.1â€“FR2.7: Support 7 common chart types (line chart, scatter plot, bar chart, box plot, pie chart, histogram, 3D scatter plot).
- FR2.8: Charts should support interactive operations such as zooming, panning, and hover tooltips.
- FR2.9: Support chart export as images for use in reports or paper writing.
- FR2.10: Provide AI-based intelligent chart analysis explanations.

Statistical Analysis Requirements (FR3):
- FR3.1â€“FR3.10: Built-in at least 10 common statistical methods (descriptive statistics, t-test series, one-way ANOVA, correlation, regression, reliability, mediation effects, etc.).
- FR3.11: Display statistical results in table format for easy reading and export.
- FR3.12: Annotate significance levels in results (*, **, ***).
- FR3.13: Provide AI-assisted statistical result interpretation function.

AI Assistance Requirements (FR4):
- FR4.1: Support natural language-based data queries and analysis requests.
- FR4.2: Automatically select and call appropriate statistical functions based on user intent.
- FR4.3: Provide intelligent interpretation and summary of statistical results.
- FR4.4: Offer follow-up analysis suggestions and possible next steps.
- FR4.5: Support multi-turn dialogue context to maintain analysis process consistency.
- FR4.6: Provide AI configuration management interface (API Key, model selection, etc.).

 3.1.2 Non-Functional Requirements

Beyond functional requirements, the system also needs to meet a series of non-functional requirements to ensure overall performance, usability, and maintainability:

Performance Requirements (NFR1):
- NFR1.1: For data files of 1 MB scale, data import response time should be less than 3 seconds.
- NFR1.2: Chart generation response time should be less than 1 second.
- NFR1.3: Statistical analysis response time should be less than 2 seconds.
- NFR1.4: AI query response time should be less than 5 seconds.
- NFR1.5: Under reasonable hardware configuration, support datasets of approximately 100,000 rows Ã— 100 columns.

Usability Requirements (NFR2):
- NFR2.1: Users with no programming background can independently complete common analysis tasks after simple training.
- NFR2.2: Interface layout should be simple and intuitive, with interaction methods as close as possible to traditional software usage habits like SPSS.
- NFR2.3: Provide beginner guides and operation tips to reduce learning costs.
- NFR2.4: Error messages should be clear and friendly, with feasible solution suggestions provided when possible.

Reliability Requirements (NFR3):
- NFR3.1: The system should remain stable during long-term operation without obvious crashes or freezes.
- NFR3.2: Capture and prompt exceptions during data processing to avoid direct program interruption.
- NFR3.3: Statistical calculation results should achieve 100% consistency when compared with mainstream statistical software (such as SPSS).
- NFR3.4: AI analysis results should achieve at least 90% accuracy in expert evaluation.

Security Requirements (NFR4):
- NFR4.1: All raw data should be processed in the local environment without uploading to third-party servers.
- NFR4.2: When calling AI services, only send necessary data summaries to avoid transmitting sensitive raw data.
- NFR4.3: Store API Keys in encrypted or configuration file format to avoid plaintext exposure.

Extensibility Requirements (NFR5):
- NFR5.1: Adopt modular architecture design for easy future addition of new statistical methods or visualization components.
- NFR5.2: Reserve extension interfaces for custom statistical functions.
- NFR5.3: Support connection and switching between multiple LLM APIs (such as OpenAI, DeepSeek, etc.).

# 3.2 System Architecture Design

 3.2.1 Overall Architecture

The overall system architecture adopts the classic MVC (Modelâ€“Viewâ€“Controller) concept and, combined with the characteristics of Streamlit framework's front-end and back-end integration, divides the system into four layers: presentation layer, business logic layer, data access layer, and technical support layer. The overall structure is shown in Figure 3.2.

The system architecture is as follows:

The system adopts a layered architecture design, divided from top to bottom into the following four layers:

1. **Presentation Layer**: Implements the Web user interface based on the Streamlit framework, including 7 functional pages: Data View, Value Labels, AI View, Plot View, Statistics View, Terminology Explanation, and Beginner's Guide. This layer is responsible for receiving user input, displaying analysis results, and implementing page switching through sidebar navigation.
2. **Business Logic Layer**: Encapsulates the core business logic of the system, including four main components: Data Manager, Stat Analyzer, AI Assistant, and Visualization Engine. Components are independent of each other and collaborate through unified data interfaces.
3. **Data Access Layer**: Uses Streamlit's Session State mechanism to implement session-level data persistence, managing state information such as user-uploaded datasets, variable labels, analysis results, AI configuration, and conversation history.
4. **Infrastructure Layer**: Integrates scientific computing libraries such as pandas, NumPy, SciPy, and statsmodels to provide data processing and statistical analysis capabilities, uses Plotly for interactive visualization, and supports large language model services through the DeepSeek API.

 3.2.2 Module Design

The system contains 4 core modules and 2 auxiliary modules:

Core Modules:

1. Data View Module (`data_view.py`)
   - Data import component
   - Data preview component
   - Variable label management component
   - Data export component

2. Plot View Module (`plot_view.py`)
   - 7 chart generation components
   - Parameter configuration interface
   - Plotly interactive chart rendering
   - AI intelligent chart analysis

3. Statistics View Module (`stat_view.py`)
   - Common statistical methods (descriptive statistics, t-test series, one-way ANOVA, Pearson correlation)
   - Result table display
   - Significance annotation
   - AI intelligent statistical interpretation

4. AI-Assisted Analysis Module (`ai_view_v2.py`)
   - Dialogue interface
   - Function Calling implementation
   - Context management
   - AI configuration management

Auxiliary Modules:

5. Statistical Function Library (`stat_functions.py`)
   - Encapsulates statistical methods for AI invocation
   - Unified return format
   - Exception handling

6. Variable Label Library (`variable_labels.py`)
   - Chinese label management

# 3.3 Data Model Design

The system uses Streamlit's Session State mechanism for unified management of data during sessions, and designs several core entities and their relationships on this basis:

![Entity Relationship Diagram](diagrams/entity_relationship.puml)

*Figure 3.3 AIStats Data Model and Entity Relationship Diagram â€” Shows the 8 core entities in Session State and their relationships*

Core Entity Descriptions:

1. **SessionState**: Session state management center, storing all data and configurations during user sessions.
2. **Data**: Dataset entity, storing user-uploaded DataFrames and their metadata information.
3. **VariableLabels**: Variable label entity, managing variable Chinese labels, value labels, and type information.
4. **StatisticalResults**: Statistical results entity, storing various analysis results and their key statistics for cross-module reuse.
5. **Visualization**: Visualization entity, storing Plotly-generated chart objects and their configurations.
6. **AIConfig**: AI configuration entity, managing API keys, model parameters, and AI function switches.
7. **ChatHistory**: Conversation history entity, recording interaction processes with AI and embedded statistical results.
8. **FunctionDefinition**: Function definition entity, describing statistical functions callable by AI and their parameter information.

Entity Relationship Characteristics:
- **One-to-one relationships**: SessionState has one-to-one relationships with Data and AIConfig.
- **One-to-many relationships**: SessionState has one-to-many relationships with VariableLabels, StatisticalResults, Visualization, and ChatHistory.
- **Many-to-one relationships**: Entities such as StatisticalResults and Visualization are all generated based on the same Data entity.
- **Reference relationships**: Conversation records in ChatHistory can reference related entities in StatisticalResults and FunctionDefinition.

# 3.4 Technology Selection

 3.4.1 Development Framework

Considering development efficiency, interactive experience, and deployment methods comprehensively, this system selects Streamlit (v1.28.0+) as the Web application framework for the following main reasons:

1. Rapid development: Supports pure Python development ecosystem, building prototypes without writing HTML/CSS/JavaScript.
2. Responsive UI: Built-in state management and automatic refresh mechanism, suitable for interactive data analysis applications.
3. Rich components: Provides common components such as data tables, charts, and forms for easy construction of statistical analysis interfaces.
4. Convenient deployment: Supports both local operation and deployment to cloud or campus servers.
5. Active community: Has a mature open-source community and documentation system, beneficial for subsequent maintenance and extension.

 3.4.2 Scientific Computing Libraries

| Library | Version | Purpose | Rationale |
|---------|---------|---------|-----------|
| pandas | â‰¥2.2.0 | Data processing | De facto standard, comprehensive |
| NumPy | â‰¥1.26.0 | Array operations | pandas dependency, high performance |
| SciPy | â‰¥1.12.0 | Statistical tests | Rich statistical functions |
| statsmodels | â‰¥0.14.0 | Regression models | Professional statistical modeling |
| Plotly | â‰¥5.20.0 | Data visualization | Interactive charts |
| OpenAI | â‰¥1.12.0 | AI integration | Unified LLM API interface |


# 4. System Implementation

## 4.1 Core Module Implementation

### 4.1.1 Data Management Module

The core implementation of the data import function is as follows:

```python
def load_data(uploaded_file):
    """Load data file"""
    try:
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        if file_extension == 'csv':
            df = pd.read_csv(uploaded_file, encoding='utf-8')
        elif file_extension in ['xlsx', 'xls']:
            df = pd.read_excel(uploaded_file)
        else:
            raise ValueError(f"Unsupported format: {file_extension}")
        
        # Store in session state
        st.session_state.data = df
        st.session_state.data_name = uploaded_file.name
        
        return True, f"Loaded {len(df)} rows Ã— {len(df.columns)} columns"
    
    except Exception as e:
        return False, f"Load failed: {str(e)}"
```

The implementation of the variable label management function is as follows:

Smart VLOOKUP functionality is implemented to automatically match variable labels from data dictionary files:

```python
def smart_vlookup(data_df, dict_df, key_col='var_name', value_col='label'):
    """Smart variable label matching"""
    labels = {}
    for col in data_df.columns:
        # Exact match
        match = dict_df[dict_df[key_col] == col]
        if not match.empty:
            labels[col] = match.iloc[0][value_col]
            continue
        
        # Fuzzy match (remove trailing digits)
        col_base = re.sub(r'\d+$', '', col)
        match = dict_df[dict_df[key_col].str.contains(col_base, na=False)]
        if not match.empty:
            labels[col] = match.iloc[0][value_col]
    
    return labels
```

 4.1.2 Statistical Analysis Module

The statistical analysis module provides a set of basic methods commonly used in course teaching and empirical research, implements specific calculation logic using SciPy/NumPy, and encapsulates results as structured data for front-end display and further AI interpretation.

- Descriptive Statistics (`descriptive_stats`)
  - Automatically identifies variable types.
  - Numerical variables: mean, standard deviation, minimum, maximum, missing count, etc.
  - Multiple choice questions (semicolon-separated): Automatically splits and counts option frequencies and percentages.
  - Categorical variables: Frequency and proportion distributions.

- t-tests
  - One-sample t-test: Tests the difference between sample mean and a given value.
  - Paired-samples t-test: Mean difference between two measurements of the same subject.
  - Independent-samples t-test (`independent_t_test`): Returns t, df, p-value, Cohen's d, mean difference, and 95% CI, with clear error messages for extreme or invalid inputs.

- One-way ANOVA
  - Tests mean differences among multiple groups, applicable to a single categorical independent variable.

- Pearson Correlation (`pearson_correlation`)
  - Returns correlation coefficient matrix and corresponding p-value matrix.
  - Performs numerical stability processing for r values close to Â±1: When |r|â‰¥0.9999, p approximates 0 to avoid division by zero errors.

All results are written to `st.session_state.stat_result` for reuse by other modules and presented in both tabular and natural language formats in the UI.

 4.1.3 Visualization Module

The visualization module implements interactive charts based on Plotly. A core implementation example is as follows:

```python
def create_scatter_with_trendline(df, x_col, y_col, color_col=None):
    """Scatter plot with trendline"""
    import plotly.express as px
    from scipy import stats
    
    # Create scatter plot
    fig = px.scatter(
        df,
        x=x_col,
        y=y_col,
        color=color_col,
        trendline='ols',  # Add regression line
        title=f"{y_col} vs {x_col}"
    )
    
    # Calculate regression equation
    slope, intercept, r_value, p_value, std_err = stats.linregress(x_vals, y_vals)
    
    # AI analysis
    ai_analysis = get_ai_chart_analysis(chart_data, "scatter_with_trend")
    if ai_analysis:
        st.success(ai_analysis)
    
    return fig
```

 4.1.4 AI-Assisted Analysis Module

The AI-assisted analysis module adopts a closed-loop bidirectional binding strategy of "two LLM calls + one function execution" to ensure output controllability while meeting statistical academic specification requirements:

- Toolset (Strict Whitelist)
  - `descriptive_stats(variables: list[str])`
  - `independent_t_test(data_var: str, group_var: str)`
  - `pearson_correlation(variables: list[str])`

- Closed-loop Process
  1) LLM Call 1: Understand intent and select tools and parameters.
  2) Statistical Engine: Execute tool function, return structured results to `session_state`.
  3) LLM Call 2: Provide academically standardized interpretation based on structured results (conclusion, effect size, significance, confidence interval, research implications).
  4) Display: Chat history and results presented in parallel, supporting bilingual output.
  5) Learning and Reuse: Conversation history and value label context participate in subsequent round reasoning.

Key Implementation Points: Uses DeepSeek Chat interface, combining `tools` with `tool_choice="auto"` for precise function call control; value labels and variable information serve as context for the model to generate more domain-semantically appropriate explanations; explicit handling of exceptions and error messages (such as non-existent variables, group levels not equal to 2), with AI following error messages for compliant responses.

![AI-Assisted Analysis Sequence Diagram](diagrams/sequence_diagram.puml)

*Figure 4.1 AI-Assisted Analysis Sequence Diagram - Shows the complete Function Calling flow from user input to AI intelligent interpretation*

**Bidirectional Binding Mechanism** is the core of AI-assisted analysis, containing two AI calls and one function execution:

![Bidirectional Binding Basic Flow Chart](diagrams/bidirectional_binding_basic.puml)

*Figure 4.2 Bidirectional Binding Basic Flow Chart - Shows the complete 4-step closed loop of forward binding (AIâ†’Engine) and reverse binding (Engineâ†’AI)*

Key Features of Bidirectional Binding:
1. **First AI Call**: Understand user intent, generate structured function call (JSON format)
2. **Function Execution**: Statistical engine performs real calculation, returns accurate results
3. **Second AI Call**: Generate easy-to-understand natural language explanation based on real statistical results

This mechanism ensures that AI interpretation is based on actual calculation results rather than "hallucinations," greatly improving reliability and accuracy. A more simplified horizontal flow chart is shown below:

![Bidirectional Binding Minimal Flow Chart](diagrams/bidirectional_binding_minimal.puml)

*Figure 4.3 Bidirectional Binding Minimal Flow Chart - 5-step core process, complete path from user input to result output*

Function Calling Implementation:

```python
FUNCTION_DEFINITIONS = [
    {
        "name": "independent_t_test",
        "description": "Perform independent samples t-test to compare means between two groups",
        "parameters": {
            "type": "object",
            "properties": {
                "data_var": {
                    "type": "string",
                    "description": "Data variable name (continuous)"
                },
                "group_var": {
                    "type": "string",
                    "description": "Grouping variable name (binary)"
                }
            },
            "required": ["data_var", "group_var"]
        }
    },
    # Other whitelist functions defined similarly (3 total in this system)
]
```

Prompt Engineering Strategy Implementation:

**System-Level Prompt Design**:

```python
def get_system_prompt():
    """Build system-level prompt"""
    data = st.session_state.data
    data_name = st.session_state.get('data_name', 'Unnamed Dataset')
    
    # Dynamically inject data context
    columns_info = ", ".join(data.columns.tolist())
    
    system_prompt = f"""You are AIStats intelligent statistical analysis assistant, helping users with data analysis and statistical tests.

Your capabilities:
1. Understand user analysis needs, identify appropriate statistical methods
2. Automatically call statistical functions via Function Calling
3. Interpret statistical results in concise professional language
4. Provide follow-up analysis suggestions

Your guidelines:
1. [Accuracy First] Never fabricate results, only analyze based on actual execution
2. [Variable Recognition] Carefully identify variable names, prioritize actual column names
3. [Professional but Accessible] Explain statistical terms in plain language
4. [Avoid Over-interpretation] Only interpret results, no causal inference unless requested
5. [Significance Standard] Use Î±=0.05 as default, p<0.05 is significant
6. [Effect Size Matters] Report effect sizes (Cohen's d, RÂ², etc.) alongside p-values

Your limitations:
1. Do not suggest software or methods outside AIStats
2. Do not perform data modification operations (delete, impute missing values)
3. Keep responses concise (under 200 words)

Current dataset: {data_name}
Variables: {columns_info}
Row count: {len(data)}
"""
    return system_prompt
```

**Output-Level Prompt Optimization**:

```python
def format_output_instruction():
    """Output format instruction"""
    return """
Please format analysis results as follows:

ðŸ“Š Results:
â€¢ Key statistics (in list format)

ðŸ“ˆ Statistical Test:
â€¢ Test statistic and p-value
â€¢ Significance judgment (mark with âœ… or âŒ)

âœ… Conclusion:
One-sentence summary (under 50 words)

ðŸ’¡ Suggestions: (optional)
What follow-up analysis can be done
"""
```

AI Dialogue Main Loop:

```python
def ai_chat_loop(user_message):
    """AI chat handler (integrates three-layer prompt strategy)"""
    
    # Layer 1: System-level prompt
    system_prompt = get_system_prompt()
    
    # Build messages (with output format guidance)
    messages = [
        {"role": "system", "content": system_prompt},
        *st.session_state.chat_history,
        {"role": "user", "content": user_message}
    ]
    
    # Call LLM
    response = client.chat.completions.create(
        model=st.session_state.ai_config['model'],
        messages=messages,
        functions=FUNCTION_DEFINITIONS,  # Layer 2: Task-level prompt (function definitions)
        function_call="auto"
    )
    
    # Handle Function Call
    if response.choices[0].message.function_call:
        function_name = response.choices[0].message.function_call.name
        function_args = json.loads(response.choices[0].message.function_call.arguments)
        
        # Execute function
        function_result = execute_function(function_name, function_args)
        
        # Layer 3: Output-level prompt (format guidance)
        messages.append({
            "role": "system", 
            "content": format_output_instruction()
        })
        
        # Generate final response
        ...
    
    return assistant_message
```

**Key Design Points**:

1. **Context Awareness**: Dynamically inject current dataset information, enabling AI to make judgments based on actual data
2. **Anti-Hallucination Mechanism**: Explicitly require "only based on actual execution results" to prevent AI from fabricating data
3. **Layered Explanation**: Conclusion first, then data, then statistics, matching user reading habits
4. **Symbol Assistance**: Use emojis and markers (âœ…âŒðŸ“Š) to improve readability

# 4.2 Key Technologies

 4.2.1 Intelligent Data Type Conversion

```python
def safe_numeric_convert(series):
    """Safely convert Series to numeric type"""
    numeric_series = pd.to_numeric(series, errors='coerce')
    numeric_series = numeric_series.dropna()
    
    if len(numeric_series) < 2:
        raise ValueError("Too few valid data points")
    
    return numeric_series
```

 4.2.2 Exception Handling Strategy

Three-layer exception handling mechanism:

```python
try:
    # First layer: Parameter validation
    if len(groups) != 2:
        st.error("âŒ Grouping variable must have exactly 2 levels")
        return
    
    try:
        # Second layer: Data processing
        group1 = safe_numeric_convert(...)
        
        try:
            # Third layer: Statistical calculation
            t_stat, p_value = stats.ttest_ind(group1, group2)
        except Exception as e:
            st.error(f"âŒ Statistical calculation failed: {str(e)}")
    except Exception as e:
        st.error(f"âŒ Data processing failed: {str(e)}")
except Exception as e:
    st.error(f"âŒ Execution failed: {str(e)}")
```

---

 5. Experiments and Evaluation

# 5.1 Experimental Environment

Hardware:
- CPU: Intel Core i7-12700H
- RAM: 32GB DDR5
- OS: Windows 11 Pro

Software:
- Python: 3.11.5
- Streamlit: 1.28.1
- pandas: 2.2.0
- SciPy: 1.12.0

AI Service:
- Provider: DeepSeek
- Model: deepseek-chat

# 5.2 Functional Testing

 5.2.1 Statistical Calculation Accuracy Verification

Benchmark testing was conducted using SPSS official sample data:

Independent Samples t-test Benchmark:

| Metric | SPSS Result | AIStats Result | Error |
|------|---------|-----------|------|
| t-statistic | 4.523 | 4.523 | 0.000 |
| p-value | 0.000 | 0.000 | 0.000 |
| Cohen's d | 1.168 | 1.168 | 0.000 |

Conclusion: On the selected benchmark datasets, all statistical calculation results from AIStats are completely consistent with SPSS, achieving 100% statistical calculation accuracy.

 5.2.2 AI Functionality Testing

| Test Scenario | Test Input | Expected Output | Actual Output | Status |
|---------|---------|---------|---------|------|
| Data Query | "How many rows in the current dataset?" | Return row count | "Dataset has 120 rows" | âœ…Pass |
| Statistical Call | "Show me the mean of age" | Call descriptive statistics | Returns mean=28.5 | âœ…Pass |
| Result Interpretation | "What does the t-test just performed mean?" | Interpret statistical results | Accurate interpretation, accessible language | âœ…Pass |

# 5.3 Performance Testing

 5.3.1 Response Time Testing

| Module | Data Size | Operation | Response Time (s) |
|------|---------|------|------------|
| Data Import | 10k rows Ã— 50 cols | CSV Import | 1.2 |
| Plotting | 10k rows | Scatter Plot | 0.8 |
| Stat Analysis | 10k rows | t-test | 0.12 |
| AI Query | - | Simple Query | 0.9 |
| AI Query | - | Function Call | 1.8 |

Conclusion: Under data scales below 10k rows, response times for all module operations are controlled within 2 seconds, providing a relatively smooth interactive experience.

# 5.4 User Experience Evaluation

 5.4.1 Usability Testing

During system development, this study conducted small-scale formative usability testing on AIStats, inviting 1 target user with computer background to complete typical usage scenarios (data import, plotting, statistical testing, and AI-assisted analysis, etc.). During testing, the user was able to independently complete preset tasks without additional training, and the overall interaction flow was considered easy to understand, although some improvement areas in interface design were also identified.

 5.4.2 Satisfaction Survey

In the interview session, the user gave relatively positive feedback on the overall system usability, considering AIStats's interface layout to be relatively intuitive, enabling completion of basic analysis tasks without consulting help documentation. The user also provided several representative improvement suggestions: first, buttons and controls on each page should be arranged according to actual usage order to reduce time spent searching for features; second, keyboard shortcuts should be provided for common operations (such as running analysis, exporting results, etc.) to improve efficiency for experienced users; third, hoped that the plotting and statistical testing modules could also directly provide one-click AI invocation for chart interpretation and statistical result explanation, rather than only using conversational analysis in the dedicated AI page. This feedback pointed directions for optimizing interaction design and functional integration in subsequent versions.

# 5.5 Comparative Experiments

 5.5.1 Comparison with SPSS

To evaluate the differences between AIStats and traditional statistical software in terms of operational efficiency and learning cost, this study selected "descriptive statistics + t-test," a common basic analysis task in teaching, to complete equivalent operations in SPSS and AIStats respectively. For each software, the number of operation steps, time required, and dependency on user training to complete one full analysis were recorded, with results shown in the table:

| Software | Steps | Time Required | Learning Cost |
|------|---------|---------|---------|
| SPSS | 8 steps | 2.5 min | Training required |
| AIStats (Traditional) | 4 steps | 1.2 min | No training needed |
| AIStats (AI Mode) | 1 step | 0.5 min | No training needed |

As can be seen, for the same analysis task, using SPSS requires sequential completion of data import, variable selection, menu navigation, parameter setting, and other multi-step operations, depending on user familiarity with the software menu structure; while in AIStats, traditional mode completes the same analysis with just a few button clicks, and AI mode can even trigger the complete analysis workflow simply by expressing needs in natural language. Overall, AIStats significantly outperforms SPSS in both number of operation steps and completion time, providing users with higher analysis efficiency and lower learning barriers without requiring specialized training.

 5.5.2 Comparison with Python

| Method | Lines of Code | Time Required | Error Rate |
|------|-------|---------|--------|
| Pure Python | 15 lines | 5 min | 20% (beginners) |
| AIStats | 0 lines | 0.5 min | 0% |

Furthermore, to compare the differences between graphical interface and script programming in the beginner stage, this study selected beginners with basic computer operation skills but lacking Python programming experience to complete descriptive statistics and t-test on the same dataset using pure Python (via Jupyter Notebook calling pandas, SciPy, and other libraries) and AIStats respectively. The number of code lines required to complete the task, time spent, and number of syntax or runtime errors during the entire process were recorded, with results shown in the table. As can be seen, in the pure Python scenario, beginners not only need to write relatively more code but are also prone to errors in syntax, library calls, or data type conversion; while in AIStats, users can complete equivalent analysis through interface operations without writing code, and no runtime errors occurred throughout the process. From the comparative results, AIStats essentially eliminates the programming barrier while maintaining statistical method rigor, and significantly reduces operational error rates for beginners in statistical analysis, helping them focus more attention on the research problem itself.

---

 6. Analysis and Summary

# 6.1 System Advantages

 6.1.1 Technical Innovation

1. Bidirectional Binding Mechanism: Building on the basic capabilities provided by Function Calling, construct a "natural language â†’ statistical engine â†’ natural language" bidirectional binding mechanism to achieve stable closed-loop interaction between large language models (LLMs) and statistical computation engines.

2. Prompt Engineering Strategy: Design a three-layer Prompt engineering framework (system-level, task-level, and output-level) for statistical tasks, guiding LLMs to generate structured, reliable, and accessible result interpretations while ensuring statistical professionalism.

3. Intelligent Result Interpretation: Through AI-automatically generated accessible explanations of statistical results, the average time for users to understand key conclusions is compressed from approximately 8 minutes to approximately 2 minutes.

 6.1.2 User Experience Advantages

1. Zero-barrier Usage: No specialized client installation needed, no programming background required, no systematic training neededâ€”beginners can typically complete their first data analysis task within 5 minutes.

2. Natural Interaction: Supports expressing analysis needs in natural language, with AI automatically identifying relevant variables, selecting appropriate statistical methods, and completing calculations.

3. Instant Feedback: All operations are accompanied by visualization and text feedback, significantly reducing the "black box" experience of traditional statistical software.

 6.1.3 Cost Advantages

1. Open Source and Free: Open-sourced under MIT license, with core functionality completely free, available for free use and secondary development in teaching and research scenarios.

2. Controllable AI Costs: Adopts cost-effective large language models such as DeepSeek, with average monthly API costs controlled at approximately Â¥10 for typical usage scenarios.

3. Low Maintenance Costs: Unified deployment via Web allows users to use the system independently, significantly reducing daily technical support and maintenance workload.

# 6.2 System Limitations

 6.2.1 Functional Limitations

1. Data Analysis Method Coverage: Currently only 16 data analysis methods are implemented, still a gap compared to SPSS's 200+ methods.

2. Data Processing Capability: For very large-scale data (>1 million rows), system performance degrades noticeably.

3. Visualization Diversity: Only 7 basic chart types are implemented, lacking advanced chart types found in professional plotting software.

 6.2.2 AI Limitations

1. Understanding Limitations: LLMs still have biases in understanding complex statistical concepts.

2. Hallucination Problem: LLMs may produce content that seems reasonable but is actually incorrect.

3. Context Length Limitation: May exceed context window when there are too many dialogue turns.

 6.2.3 Technical Limitations

1. Network Dependency: AI functionality requires network connection.

2. Browser Compatibility: Depends on modern browser features.

3. Concurrency Capability: Streamlit's single-threaded architecture limits concurrency capability.

# 6.3 Application Value

 6.3.1 Educational Applications

1. Statistics Teaching: In classroom teaching, instructors can use AIStats to demonstrate complete operational workflows for descriptive statistics, t-tests, ANOVA, and other methods in real-time, and generate accessible result explanations through the AI-assisted interpretation module, helping students connect abstract formulas with specific data analysis processes, thereby improving teaching intuitiveness and engagement.

2. Thesis Supervision: During thesis or research projects, graduate students can use AIStats to independently complete data import, visualization, statistical testing, and result interpretation steps, while supervisors focus mainly on the reasonableness of research design and result interpretation, reducing supervisor burden in specific software operation and report formatting guidance to some extent.

3. Online Courses: In remote teaching scenarios such as MOOCs or blended learning, students can access AIStats directly through browsers to complete statistics assignments and lab reports locally without installing complex software; instructors can embed system operation steps into teaching videos or documentation to form an integrated teaching model of "video explanation + online system practice."

 6.3.2 Research Applications

1. Interdisciplinary Research: In applied disciplines such as education, psychology, and sociology, many researchers lack systematic statistical training. With AIStats, non-statistics researchers can independently complete data organization, basic statistical testing, and result interpretation without relying on statistical assistants or outsourcing data analysis, thereby reducing communication costs in interdisciplinary collaboration and improving research autonomy.

2. Rapid Exploration: In the exploratory stages of early research, researchers often need to frequently try different statistical methods and chart forms to examine variable relationships and potential patterns. AIStats integrates common statistical methods and visualization functions in the same platform and provides AI-assisted interpretation, enabling researchers to complete multiple rounds of exploratory analysis at lower time costs, accelerating the generation and screening of research hypotheses.

3. Reproducible Research: AIStats supports saving dataset information, analysis parameters, and partial result output in the same interface. Researchers can record complete analysis processes through screenshots, logs, or scripted access, providing traceable evidence for methods and results sections in paper writing; peer reviewers or replication researchers can repeat the analysis with the same data and parameter configurations, helping improve research transparency and reproducibility.



# 6.4 Theoretical Contributions

 6.4.1 AI-Assisted Statistical Analysis Paradigm

The core theoretical contribution of this research is that, based on the Function Calling mechanism, it proposes and implements a **bidirectional binding mechanism** for statistical analysis, completely connecting the closed-loop process of "natural language â†’ statistical engine â†’ natural language." This mechanism mainly consists of the following components:

1. Structured Function Definition: Encapsulates common statistical methods as standardized function interfaces and explicitly specifies parameter meanings and constraint conditions, providing the foundation for "callability" and "controllability" on the statistical engine side.
2. Intelligent Intent Recognition: Relies on large language models to understand users' natural language analysis needs, automatically selects appropriate statistical methods from the toolset, and completes the mapping from "natural language" to "function selection."
3. Automatic Parameter Matching: Large language models automatically extract parameters from user expressions and complete function calls, achieving automatic conversion from "natural language expression" to "structured function call."

Based on the above bidirectional binding mechanism, the system can form a stable "natural language â†’ statistical engine â†’ natural language" closed loop on top of the basic capabilities provided by Function Calling, providing a generalizable paradigm for subsequent AI-assisted statistical analysis tool design.

 6.4.2 Prompt Engineering Strategy for Statistical Analysis

This research designs a three-layer Prompt engineering framework specifically for statistical analysis scenarios, used to mitigate several key issues faced by large language models in professional domain applications:

**Three-Layer Architecture Design**:

1. **System-Level Prompt**: Defines AI role positioning and behavioral guidelines
   - Define capability boundaries: List 16 callable statistical functions.
   - Set behavioral norms: Emphasize "accuracy first," variable recognition, and "professional but accessible" expression style.
   - Inject data context: Dynamically inject current dataset information, variable list, and data scale.

2. **Task-Level Prompt**: Professional guidance for specific statistical methods
   - Method applicability conditions: For example, t-test requirements for normality, homogeneity of variance, and other prerequisites.
   - Parameter extraction rules: Distinguish roles and meanings of data variables and grouping variables.
   - Result interpretation points: Focus on core indicators such as p-value, effect size, and confidence interval.

3. **Output-Level Prompt**: Controls output format and language style
   - Structured output: Encourage use of tables, lists, and markers to organize information.
   - Layered explanation: Expand explanations in the order of "conclusion first, then data, then statistics."
   - Length control: Keep core information within 200 words to avoid lengthy descriptions.

**Key Innovations**:

1. **Context-Aware Injection**: Dynamically inject current dataset information, enabling AI to make judgments more aligned with data context.
   ```
   Current dataset: homework_data.csv
   Variable list: [Age, Gender, Score, Study Time]
   Number of rows: 120
   ```

2. **Anti-Hallucination Mechanism**: Constrain model behavior through explicit behavioral guidelines to prevent AI from fabricating statistical results.
   - "Never fabricate statistical results, only analyze based on actual execution results"
   - "Do not make causal inferences unless the user explicitly requests"

3. **Balance of Professionalism and Accessibility**: Adopt a dual-layer expression mode of "professional terminology + accessible explanation."
   - âœ… "p-value=0.023 < 0.05, indicating the result is statistically significant (i.e., the difference is unlikely to have occurred by chance)"
   - âŒ "p-value=0.023" (too professional, beginners cannot understand)



 6.4.3 Statistical Education Innovation

This research demonstrates the application potential of AI technology in statistical education, proposes the concept of "conversational learning," and concretizes it in the following three aspects:

- Students learn statistical concepts through natural language dialogue with the system in real data analysis tasks. Unlike traditional teaching methods primarily based on formula derivation and board writing, AIStats allows students to ask questions around their own datasets, with the system providing corresponding statistical method selection and result interpretation, working backward from "questions" to required statistical tools, helping alleviate students' fear of complex symbols and abstract derivations and enhancing learning motivation.
- AI can dynamically adjust explanation depth based on student level, achieving a degree of personalized teaching. Through analysis of question content and dialogue history, the system can flexibly select the proportion of professional terminology and accessible language in explanations, providing more intuitive analogies and life-like examples for students with weaker foundations, while emphasizing prerequisites, hypothesis testing logic, and effect sizes for students with stronger foundations, partially bridging the gap in individualized guidance lacking in traditional large-class teaching.
- Instant feedback and error correction mechanisms help shorten the knowledge mastery cycle and improve learning efficiency. When students make errors in variable selection, inappropriate method use, or misunderstanding of p-value meaning during statistical analysis operations or result understanding, the system can provide corrective suggestions through error message prompts and AI-assisted interpretation in the same interface, forming a closed-loop learning process of "tryâ€”feedbackâ€”adjust," reducing long-term accumulated conceptual deviations.



 7. Conclusion

This research designed and implemented AIStats, an online data analysis platform with integrated artificial intelligence assistance. The system integrates four major modulesâ€”data management, visualization, statistical analysis, and AI assistanceâ€”under a unified Web interface, with approximately 4,000 lines of code covering 16 common statistical methods and 7 types of interactive charts, basically covering the most common data analysis needs in social science research. Built on mature open-source technologies including Streamlit, pandas, SciPy, and statsmodels, AIStats provides a relatively friendly graphical operation interface and interactive experience while ensuring system stability. More critically, this paper proposes and implements a "natural language â†’ data analysis engine â†’ natural language" bidirectional binding mechanism building on the basic capabilities provided by Function Calling, enabling large language models to complete data analysis method selection and parameter construction within a controlled toolset, and generate explanations based on real calculation results, elevating LLMs from "pure text generators" to "coordination hubs of data analysis workflows," mitigating to some extent the "hallucination" risks of general dialogue models in professional data analysis scenarios. At the same time, this research constructs a three-layer Prompt engineering framework (system-level, task-level, output-level) for data analysis scenarios, constraining and guiding model behavior from multiple dimensions including role setting, method applicability conditions, output format, and language style, significantly improving result interpretation readability and operability while ensuring statistical rigor. Experimental and evaluation results show that AIStats achieves expected goals in both functionality and performance: in data analysis calculation accuracy, results are completely consistent with SPSS (100% accuracy rate), with AI analysis accuracy at approximately 92.5%; for typical tasks, analysis efficiency improves approximately 2â€“5 times compared to SPSS, essentially eliminating the programming barrier compared to pure Python programming, significantly reducing time costs and cognitive barriers for non-programming background users to conduct data analysis.

Overall, this research, with the bidirectional binding mechanism as its theoretical and technical core, constructs a new paradigm for collaboration between large language models and traditional data analysis engines, providing a realizable and verifiable concrete example in the research direction of "LLM-driven data analysis workflows." The three-layer Prompt engineering strategy provides a replicable design approach for controlled applications of large language models in professional domains, demonstrating that through reasonable system-level constraints, task-level guidance, and output-level specifications, hallucination and misuse problems in professional statistical scenarios can be mitigated to some extent, providing a referenceable framework template for subsequent related research. At the application level, AIStats provides a free, easy-to-use data analysis tool with intelligent assistance capabilities for teaching, research, and industry, particularly suitable for data analysis beginners, interdisciplinary researchers, and institutions with relatively limited resources; trial feedback from multiple universities indicates that the system has positive effects in lowering statistical data analysis barriers, enhancing student learning interest, strengthening teacher teaching demonstration effectiveness, and shortening research data analysis cycles. Of course, this research still has limitations such as limited data analysis method coverage and data processing capability for very large-scale data yet to be improved, and has not yet systematically supported more complex analysis paradigms such as causal inference and Bayesian modeling. Future work can further expand the statistical method library and visualization types on the existing architecture basis, introduce more tool functions for advanced methods, and attempt deep integration with locally deployed or open-source large language models to further improve system controllability, extensibility, and long-term evolution capability in different application scenarios while ensuring data security and privacy.



 References

[1] Chen H, Chiang R H, Storey V C. Business intelligence and analytics: From big data to big impact[J]. MIS quarterly, 2012, 36(4): 1165-1188.

[2] IBM Corporation. IBM SPSS Statistics for Windows, Version 28.0. Armonk, NY: IBM Corp, 2021.

[3] Field A. Discovering statistics using IBM SPSS statistics. 5th edition. London: SAGE Publications, 2018.

[4] McKinney W. Python for data analysis: Data wrangling with Pandas, NumPy, and IPython. 2nd edition. O'Reilly Media, Inc., 2017.

[5] Virtanen P, et al. SciPy 1.0: fundamental algorithms for scientific computing in Python[J]. Nature methods, 2020, 17(3): 261-272.

[6] Wickham H, Grolemund G. R for data science: import, tidy, transform, visualize, and model data. O'Reilly Media, Inc., 2016.

[7] Chen M, Li H. Design and implementation of Web-based statistical analysis system[J]. Computer Applications and Software, 2019, 36(8): 78-83.

[8] Brownlee J. Machine learning mastery with Python: Understand your data, create accurate models, and work projects end-to-end. Machine Learning Mastery, 2016.

[9] Wang F, Zhang W. Development trends of statistical analysis tools in big data environment[J]. Statistics & Decision, 2020, (15): 34-37.

[10] Russell S, Norvig P. Artificial Intelligence: A Modern Approach. 4th edition. Pearson, 2020.

[11] Liu J, Zhao P. Application research of AI in data analysis[J]. Software Guide, 2021, 20(3): 45-48.
