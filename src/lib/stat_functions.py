"""ç»Ÿè®¡åˆ†æå‡½æ•°åº“ï¼Œä¾› AI è°ƒç”¨"""
import streamlit as st
import pandas as pd
import numpy as np
from scipy import stats
from src.lib.fuzzy_match import find_variable_by_keyword

def independent_t_test(data_var: str, group_var: str):
    """ç‹¬ç«‹æ ·æœ¬ t æ£€éªŒ"""
    if st.session_state.data is None:
        return {"error": "æœªå¯¼å…¥æ•°æ®"}
    
    df = st.session_state.data
    
    if data_var not in df.columns or group_var not in df.columns:
        return {"error": f"å˜é‡ {data_var} æˆ– {group_var} ä¸å­˜åœ¨"}
    
    groups = df[group_var].unique()
    if len(groups) != 2:
        return {"error": "åˆ†ç»„å˜é‡å¿…é¡»æ°å¥½æœ‰ 2 ä¸ªæ°´å¹³"}
    
    group1 = df[df[group_var] == groups[0]][data_var].dropna()
    group2 = df[df[group_var] == groups[1]][data_var].dropna()
    
    # t æ£€éªŒ
    t_stat, p_value = stats.ttest_ind(group1, group2)
    
    # Cohen's d
    pooled_std = np.sqrt(((len(group1)-1)*group1.std()**2 + (len(group2)-1)*group2.std()**2) / (len(group1)+len(group2)-2))
    cohens_d = (group1.mean() - group2.mean()) / pooled_std
    
    # ç½®ä¿¡åŒºé—´
    mean_diff = group1.mean() - group2.mean()
    se_diff = pooled_std * np.sqrt(1/len(group1) + 1/len(group2))
    ci_lower = mean_diff - 1.96 * se_diff
    ci_upper = mean_diff + 1.96 * se_diff
    
    result = {
        "test_type": "ç‹¬ç«‹æ ·æœ¬ t æ£€éªŒ",
        "data_var": data_var,
        "group_var": group_var,
        "group1_name": str(groups[0]),
        "group2_name": str(groups[1]),
        "group1_n": len(group1),
        "group2_n": len(group2),
        "group1_mean": float(group1.mean()),
        "group2_mean": float(group2.mean()),
        "group1_std": float(group1.std()),
        "group2_std": float(group2.std()),
        "mean_diff": float(mean_diff),
        "t_statistic": float(t_stat),
        "df": len(group1) + len(group2) - 2,
        "p_value": float(p_value),
        "ci_95_lower": float(ci_lower),
        "ci_95_upper": float(ci_upper),
        "cohens_d": float(cohens_d),
        "significant": "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
    }
    
    # ä¿å­˜åˆ° session_state
    st.session_state.stat_result = result
    
    return result

def descriptive_stats(variables: list):
    """æè¿°ç»Ÿè®¡ - è‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†å¤šé€‰é¢˜ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…å˜é‡å"""
    if st.session_state.data is None:
        return {"error": "æœªå¯¼å…¥æ•°æ®"}
    
    df = st.session_state.data
    
    results = {}
    for var in variables:
        # å…ˆå°è¯•ç²¾ç¡®åŒ¹é…ï¼Œå¤±è´¥åˆ™æ¨¡ç³ŠåŒ¹é…
        original_var = var
        was_fuzzy_matched = False
        
        if var not in df.columns:
            # å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
            matched_var = find_variable_by_keyword(var)
            if matched_var:
                var = matched_var  # ä½¿ç”¨åŒ¹é…åˆ°çš„å®Œæ•´å˜é‡å
                was_fuzzy_matched = True
            else:
                results[original_var] = {"error": f"å˜é‡ '{original_var}' ä¸å­˜åœ¨ï¼ˆä¹Ÿæœªæ‰¾åˆ°åŒ¹é…çš„å˜é‡ï¼‰"}
                continue
        
            # ç¡®è®¤å˜é‡å­˜åœ¨åè¿›è¡Œç»Ÿè®¡
        if var in df.columns:
            # å¦‚æœä½¿ç”¨äº†æ¨¡ç³ŠåŒ¹é…ï¼Œåœ¨ç»“æœä¸­æ ‡æ³¨
            matched_info = f" (åŒ¹é…: {original_var} â†’ {var})" if was_fuzzy_matched else ""
            data = df[var].dropna()
            
            # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœæ•°æ®ä¸ºç©ºï¼Œè¿”å›é”™è¯¯
            if len(data) == 0:
                results[var] = {
                    "type": "empty",
                    "error": "å˜é‡ä¸­æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼ˆå…¨éƒ¨ä¸ºç¼ºå¤±å€¼ï¼‰",
                    "n": 0,
                    "missing": int(df[var].isnull().sum())
                }
                continue
            
            # ğŸ” è‡ªåŠ¨æ£€æµ‹å¤šé€‰é¢˜ï¼ˆåŒ…å«åˆ†å·åˆ†éš”ï¼‰
            is_multiple_choice = False
            try:
                sample = data.head(20).astype(str)
                # æ£€æµ‹æ˜¯å¦æœ‰åˆ†å·åˆ†éš”çš„æ¨¡å¼
                if sample.str.contains(';', regex=False).any():
                    is_multiple_choice = True
            except:
                is_multiple_choice = False
            
            # è·å–å€¼æ ‡ç­¾ï¼ˆå¦‚æœæœ‰ï¼‰
            from src.lib.variable_labels import get_value_labels
            value_labels = get_value_labels(var)
            
            # æ™ºèƒ½åˆ¤æ–­ï¼šæ•°å€¼å‹å˜é‡ä½†è®¾ç½®äº†å€¼æ ‡ç­¾ â†’ å½“ä½œåˆ†ç±»å˜é‡å¤„ç†
            # æˆ–è€…ï¼šå”¯ä¸€å€¼å¾ˆå°‘ï¼ˆâ‰¤15ä¸ªï¼‰â†’ ä¹Ÿå½“ä½œåˆ†ç±»å˜é‡
            is_categorical_numeric = False
            if df[var].dtype in ['int64', 'float64']:
                unique_count = df[var].nunique()
                if value_labels or unique_count <= 15:
                    is_categorical_numeric = True
            
            # æ•°å€¼å‹å˜é‡ï¼ˆè¿ç»­å‹ï¼‰
            if df[var].dtype in ['int64', 'float64'] and not is_categorical_numeric:
                try:
                    stats_dict = {
                        "type": "numeric",
                        "n": len(data),
                        "mean": float(data.mean()),
                        "std": float(data.std()),
                        "min": float(data.min()),
                        "q1": float(data.quantile(0.25)),
                        "median": float(data.median()),
                        "q3": float(data.quantile(0.75)),
                        "max": float(data.max()),
                        "missing": int(df[var].isnull().sum())
                    }
                except Exception as e:
                    stats_dict = {
                        "type": "numeric",
                        "error": f"è®¡ç®—æ•°å€¼ç»Ÿè®¡æ—¶å‡ºé”™: {str(e)}",
                        "n": len(data),
                        "missing": int(df[var].isnull().sum())
                    }
            
            # ğŸ¯ å¤šé€‰é¢˜å¤„ç†
            elif is_multiple_choice:
                all_options = []
                valid_responses = 0
                
                for value in data:
                    if pd.notna(value):
                        valid_responses += 1
                        options = [opt.strip() for opt in str(value).split(';')]
                        all_options.extend(options)
                
                # é¿å…é™¤é›¶é”™è¯¯
                if valid_responses > 0:
                    option_counts = pd.Series(all_options).value_counts()
                    percentages = (option_counts / valid_responses * 100).round(2)
                    avg_per_person = round(len(all_options) / valid_responses, 2)
                else:
                    option_counts = pd.Series(dtype=int)
                    percentages = pd.Series(dtype=float)
                    avg_per_person = 0
                
                stats_dict = {
                    "type": "multiple_choice",  # æ ‡è®°ä¸ºå¤šé€‰é¢˜
                    "n": valid_responses,
                    "n_selections": len(all_options),
                    "avg_per_person": avg_per_person,
                    "option_frequencies": option_counts.to_dict(),
                    "option_percentages": percentages.to_dict(),
                    "missing": int(df[var].isnull().sum())
                }
            
            # æ™®é€šåˆ†ç±»å˜é‡ï¼ˆåŒ…æ‹¬è®¾ç½®äº†å€¼æ ‡ç­¾çš„æ•°å€¼å‹å˜é‡ï¼‰
            else:
                try:
                    # å®‰å…¨è·å–value_counts
                    value_counts = df[var].value_counts(dropna=True)
                    
                    # value_labels å·²åœ¨ä¸Šé¢è·å–è¿‡äº†
                    # åˆå¹¶ï¼šæ•°æ®ä¸­çš„å€¼ + æ ‡ç­¾ä¸­å®šä¹‰çš„å€¼
                    all_possible_values = set(value_counts.keys())
                    if value_labels:
                        all_possible_values.update(value_labels.keys())
                    
                    # ä¸ºæ‰€æœ‰å¯èƒ½çš„å€¼åˆ›å»ºå®Œæ•´çš„é¢‘æ¬¡å­—å…¸ï¼ˆåŒ…æ‹¬é¢‘æ¬¡ä¸º0çš„ï¼‰
                    complete_values = {}
                    complete_percentages = {}
                    
                    # å®‰å…¨æ’åºï¼šæ£€æŸ¥æ‰€æœ‰å€¼æ˜¯å¦ä¸ºæ•°å­—ç±»å‹
                    all_numeric = all(isinstance(v, (int, float)) and not isinstance(v, bool) for v in all_possible_values)
                    
                    if all_numeric:
                        # æ‰€æœ‰å€¼éƒ½æ˜¯æ•°å­—ï¼Œç›´æ¥æ•°å€¼æ’åº
                        sorted_values = sorted(all_possible_values)
                    else:
                        # åŒ…å«éæ•°å­—ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²åæ’åº
                        sorted_values = sorted(all_possible_values, key=str)
                    
                    for val in sorted_values:
                        count = value_counts.get(val, 0)
                        complete_values[val] = int(count)
                        if len(data) > 0:
                            complete_percentages[val] = round((count / len(data) * 100), 2)
                        else:
                            complete_percentages[val] = 0.0
                    
                    stats_dict = {
                        "type": "categorical",
                        "n": len(data),
                        "unique": int(df[var].nunique()),
                        "all_values": complete_values,  # å®Œæ•´çš„å€¼é¢‘æ¬¡ï¼ˆåŒ…æ‹¬0ï¼‰
                        "percentages": complete_percentages,
                        "value_labels": value_labels,  # åŒ…å«å€¼æ ‡ç­¾
                        "missing": int(df[var].isnull().sum())
                    }
                except Exception as e:
                    # å¦‚æœå‡ºé”™ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
                    stats_dict = {
                        "type": "categorical",
                        "n": len(data),
                        "unique": int(df[var].nunique()),
                        "error": f"å¤„ç†åˆ†ç±»å˜é‡æ—¶å‡ºé”™: {str(e)}",
                        "missing": int(df[var].isnull().sum())
                    }
            
            results[var] = stats_dict
    
    st.session_state.stat_result = results
    return results

def pearson_correlation(variables: list):
    """Pearson ç›¸å…³åˆ†æ"""
    if st.session_state.data is None:
        return {"error": "æœªå¯¼å…¥æ•°æ®"}
    
    df = st.session_state.data
    
    for var in variables:
        if var not in df.columns:
            return {"error": f"å˜é‡ {var} ä¸å­˜åœ¨"}
    
    # ç›¸å…³çŸ©é˜µ
    corr_matrix = df[variables].corr()
    
    # è®¡ç®— p å€¼çŸ©é˜µ
    n = len(df[variables].dropna())
    p_matrix = {}
    
    for var1 in variables:
        p_matrix[var1] = {}
        for var2 in variables:
            if var1 != var2:
                r = corr_matrix.loc[var1, var2]
                # é˜²æ­¢é™¤é›¶é”™è¯¯ï¼šå½“ræ¥è¿‘Â±1æ—¶ï¼Œ1-rÂ²æ¥è¿‘0
                if abs(r) >= 0.9999:
                    # å®Œå…¨ç›¸å…³æˆ–å®Œå…¨è´Ÿç›¸å…³ï¼Œpå€¼æå°
                    p_matrix[var1][var2] = 0.0
                else:
                    t = r * np.sqrt(n - 2) / np.sqrt(1 - r**2)
                    p = 2 * (1 - stats.t.cdf(abs(t), n - 2))
                    p_matrix[var1][var2] = float(p)
            else:
                p_matrix[var1][var2] = 1.0
    
    result = {
        "test_type": "Pearson ç›¸å…³åˆ†æ",
        "variables": variables,
        "n": n,
        "correlation_matrix": corr_matrix.to_dict(),
        "p_value_matrix": p_matrix
    }
    
    st.session_state.stat_result = result
    return result

